{"cells":[{"cell_type":"markdown","metadata":{"id":"Oh5GbEYsq5wl"},"source":["# Text Analysis\n","-----------------"]},{"cell_type":"markdown","metadata":{"id":"cOV_4E7Hq5wo"},"source":["Written by: Daniela Hochfellner, Aidan Feldman"]},{"cell_type":"markdown","metadata":{"id":"s2v28benq5wp"},"source":["Text analysis is used to extract useful information from or summarize a large amount of unstructured text stored in documents. This opens up the opportunity of using text data alongside more conventional data sources (e.g. surveys and administrative data). The goal of text analysis is to take a large corpus of complex and unstructured text data and extract important and meaningful messages in a comprehensible way. \n","\n","Text analysis can help with the following tasks:\n","\n","* **Information Retrieval**: Find relevant information in a large database, such as a systematic literature review, that would be very time-consuming for humans to do manually. \n","\n","* **Clustering and Text Categorization**: Summarize a large corpus of text by finding the most important phrases, using methods like topic modeling. \n","\n","* **Text Summarization**: Create category-sensitive text summaries of a large corpus of text. \n","\n","* **Machine Translation**: Translate documents from one language to another. \n","\n","In this notebook, we are going to analyze abstracts of grants using topic modeling to examine the content of our data. \n","\n","## Learning Outcomes\n","\n","In this notebook, you will...\n","* Learn how to transform a corpus of text into a structured matrix format so that we can apply natural language processing (NLP) methods\n","* Learn the basics and applications of topic modeling\n"," \n","## Glossary of Terms\n","\n","Glossary of Terms:\n","\n","* **Corpus**: A corpus is the set of all text documents used in your analysis; for example, your corpus of text may include hundreds of research articles.\n","\n","* **Tokenize**: Tokenization is the process by which text is separated into meaningful terms or phrases. In English this is easy to do for individual words, as they are separated by whitespace; however, it can get more complicated to  automate determining which groups of words constitute meaningful phrases. \n","\n","* **Stemming**: Stemming is normalizing text by reducing all forms or conjugations of a word to the word's most basic form. In English, this can mean making a rule of removing the suffixes \"ed\" or \"ing\" from the end of all words, but it gets more complex. For example, \"to go\" is irregular, so you need to tell the algorithm that \"went\" and \"goes\" stem from a common lemma, and should be considered alternate forms of the word \"go.\"\n","\n","* **TF-IDF**: TF-IDF (term frequency-inverse document frequency) is an example of feature engineering where the most important words are extracted by taking into account their frequency in documents and the entire corpus of documents as a whole.\n","\n","* **Topic Modeling**: Topic modeling is an unsupervised learning method where groups of words that often appear together are clustered into topics. Typically, the words in one topic should be related and make sense (e.g. boat, ship, captain). Individual documents can fall under one topic or multiple topics. \n","\n","* **LDA**: LDA (Latent Dirichlet Allocation) is a type of probabilistic model commonly used for topic modeling. \n","\n","* **Stop Words**: Stop words are words that have little semantic meaning but occur very frequently, like prepositions, articles and common nouns. For example, every document (in English) will probably contain the words \"and\" and \"the\" many times. You will often remove them as part of preprocessing using a list of stop words.\n","\n","You'll learn about feature engineering and unsupervised learning when you read about machine learning."]},{"cell_type":"markdown","metadata":{"id":"NZ5XQ49Mq5wq"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95eSYeYTq5wq"},"outputs":[],"source":["# Install package for natural language processing\n","%pip install --quiet nltk\n","\n","# data manipulation\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","# text analysis tools\n","import nltk\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn import preprocessing\n","from nltk import SnowballStemmer\n","import string"]},{"cell_type":"markdown","metadata":{"id":"PzrEdxX2q5ws"},"source":["## Motivation: Grant Proposal Abstracts\n","\n","One of the pieces of data you have access to in this class is the Federal RePORTER data, containing information about grants. In this notebook, we will use text analysis using the grant proposal abstracts in order to determine what types of topics these abstracts are about. Note that we do not have a pre-compiled list of topics that we are matching to the abstracts. Instead, we want to explore the data and use unsupervised learning in order to generate the topics.\n","\n","### Load the Data\n","\n","To start, we'll load the data into a pandas DataFrame from our class folder. We will be analyzing the abstracts of grant proposals. For demonstration purposes of this class we will only use year 2015 and we will load 2,000 abstracts (in order to be able to run the model faster in class, but you can load the full dataset in your own time, to look at the results). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DJRkj41q5wt"},"outputs":[],"source":["# Specify the path for the folders where the data is present\n","# For Windows users:\n","path = \"C:/Users/PC Name/\"\n","# For Mac users - skip the path variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQUbbFOAq5wt"},"outputs":[],"source":["# Read-in a CSV file\n","# Windows users\n","abstracts = pd.read_csv(path + 'Grants_Abstracts/Grants_Abstracts/FedRePORTER_PRJABS_C_FY2015.csv', nrows=2000, skipinitialspace=True)\n","# Mac users - uncomment this line and run the code\n","# abstracts = pd.read_csv('Grants_Abstracts/FedRePORTER_PRJABS_C_FY2015.csv', nrows=2000, skipinitialspace=True)"]},{"cell_type":"markdown","metadata":{"id":"fjw53Zt4q5wu"},"source":["## Explore the Data\n","Our text data table has 2 fields:\n","\n","- `project_id` - project ID.\n","- `abstract` - text of abstract.\n","\n","Let's take a look at examples of the values:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xWi36qkq5wu","outputId":"df9983a3-b97c-4262-dec8-18ca9c1be5e3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PROJECT_ID</th>\n","      <th>ABSTRACT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>673465</td>\n","      <td>This project has four broad objectives, linked...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PROJECT_ID                                           ABSTRACT\n","0      673465  This project has four broad objectives, linked..."]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["abstracts.head(1)"]},{"cell_type":"markdown","metadata":{"id":"s3SYLZmCq5wv"},"source":["We will save the values of the column `abstract` to a list."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycu4jTsDq5wv"},"outputs":[],"source":["abstracts_list = abstracts['ABSTRACT'].values.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HGScS57q5wv","outputId":"ac532efe-b57e-4549-c196-b0b4c7daeace"},"outputs":[{"data":{"text/plain":["'This project has four broad objectives, linked to feasible improvements in clean stove design and dissemination and their impacts on health and climate: 1) assess the acceptability and availability of different stove technologies and fuels, 2) experiment by varying stove price and social interactions among users to determine the impact of these variables on stove adoption rates, 3) measure in situ the impacts of stove adoption on indoor and outdoor air pollution, and climate-forcing, and 4) model the impacts of widespread stove adoption on regional and global climate through a range of scenarios directly informed by data from the field.'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#look at first abstract\n","abstracts_list[0]"]},{"cell_type":"markdown","metadata":{"id":"SRve748Pq5ww"},"source":["## Topic Modeling\n","We are going to apply topic modeling, an unsupervised learning method, to our corpus to find the high-level topics in our corpus. Through this process, we'll discuss how to clean and preprocess our data to get the best results. Topic modeling is a broad subfield of machine learning and natural language processing. We are going to focus on a common modeling approach called Latent Dirichlet Allocation (LDA).\n","\n","To use topic modeling, we first have to assume that topics exist in our corpus, and that some small number of these topics can \"explain\" the corpus. Topics in this context refer to words from the corpus, in a list that is ranked by probability. A single document can be explained by multiple topics. For instance, an article on net neutrality would fall under the topic \"technology\" as well as the topic \"politics\". The set of topics used by a document is known as the document's allocation, hence, the name Latent Dirichlet Allocation, each document has an allocation of latent topics allocated by Dirichlet distribution. Latent (or hidden) stands for topics in the documents that are existing but not yet developed or manifest and which can be discovered based on observed data, such as words in the documents. Dirichlet refers to distributions that are taken into account when creating topics: a distribution of words in the topic (which words are more or less probable to belong to a given topic) and a distribution of topics in documents (which topic is more or less probable for a given document). \n","\n","We will use topic modeling in order to determine what types of research is done based on the grants' abstracts."]},{"cell_type":"markdown","metadata":{"id":"--0784vKq5ww"},"source":["LDA model takes as input a corpus (a collection of text documents). Every text document is tokenized to become a sequence of words (tokens). All unique words across a given corpus are saved as a vocabulary. Text documents are then converted to a matrix of token counts (how often a given unique word from a vocabulary appears in a given text document), e.g.:\n","\n","|doc# / unique words | 'science' | 'research' | 'cell' | 'DNA' | 'gene' |\n","|-----------|--------|---------|-------|-------|------|\n","|document 1 |    0   |    0    |   1   |   5   |  7   |\n","|document 2 |    1   |    2    |   0   |   1   |  0   |\n","|document 3 |    1   |    5    |   2   |   0   |  0   |\n","\n","\n","The LDA model finds the probability of a word appearing in a given topic, and then maps a probability of a topic being assigned to a given document."]},{"cell_type":"markdown","metadata":{"id":"nrWliMKdq5ww"},"source":["### Preparing Text Data for Natural Language Processing (NLP)\n","\n","The first important step in working with text data is cleaning and processing the data, which includes (but is not limited to):\n","- forming a corpus of text\n","- stemming and lemmatization\n","- tokenization\n","- removing stopwords\n","- finding words co-located together (N-grams)\n","\n","The ultimate goal is to transform our text data into a form an algorithm can work with, because a document or a corpus of text cannot be fed directly into an algorithm. Algorithms expect numerical feature vectors with certain fixed sizes, and can't handle documents, which are basically sequences of symbols with variable length. We will be transforming our text corpus into a _bag of n-grams_ to be further analyzed. In this form our text data is represented as a matrix where each row refers to a specific job description (document) and each column is the occurence of a word (feature)."]},{"cell_type":"markdown","metadata":{"id":"l50KfO0qq5wx"},"source":["### Stemming and Lemmatization - Distilling text data"]},{"cell_type":"markdown","metadata":{"id":"SKLzImOpq5wx"},"source":["We want to process our text through _stemming and lemmatization_, or replacing words with their root or simplest form. For example, \"systems\", \"systematic\", and \"system\" are all different words, but we can replace all these words with \"system\" without sacrificing much meaning. \n","- A **lemma** is the original dictionary form of a word (e.g. the lemma for \"lies\", \"lied\", and \"lying\" is \"lie\").\n","- The process of turning a word into its simplest form is **stemming**. There are several well-known stemming algorithms -- Porter, Snowball, Lancaster - that all have their respective strengths and weaknesses.\n","\n","In this notebook, we will use the Snowball Stemmer:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEfkFH3eq5wx","outputId":"e699a992-3a69-4d1e-e9b8-96ae935c30ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["lie\n","lie\n","systemat\n","run\n"]}],"source":["# Examples of how a Stemmer works:\n","stemmer = SnowballStemmer(\"english\")\n","print(stemmer.stem('lies'))\n","print(stemmer.stem('lying'))\n","print(stemmer.stem('systematic'))\n","print(stemmer.stem('running'))"]},{"cell_type":"markdown","metadata":{"id":"SocBIwTQq5wx"},"source":["### Removing Punctuation"]},{"cell_type":"markdown","metadata":{"id":"lmBvqUUAq5wy"},"source":["For some purposes, we might want to preserve punctuation. For example, if we wanted to be able to detect sentiment of text, we might want to keep exclamation points, because they signify something about the text. For our purposes, however, we will simply strip the punctuation so that it does not affect our analysis. To do this, we use `string` package, creating a translator that takes any string and \"translates\" it into a string without any punctuation."]},{"cell_type":"markdown","metadata":{"id":"Ix3QGGIiq5wy"},"source":["An example using the first abstract in our corpus is shown below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oaS4UfJq5wy","outputId":"bbe8f987-2afb-4cca-b8f7-29c6c4367068"},"outputs":[{"data":{"text/plain":["'This project has four broad objectives, linked to feasible improvements in clean stove design and dissemination and their impacts on health and climate: 1) assess the acceptability and availability of different stove technologies and fuels, 2) experiment by varying stove price and social interactions among users to determine the impact of these variables on stove adoption rates, 3) measure in situ the impacts of stove adoption on indoor and outdoor air pollution, and climate-forcing, and 4) model the impacts of widespread stove adoption on regional and global climate through a range of scenarios directly informed by data from the field.'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Before\n","abstracts_list[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOTw6D-qq5wy"},"outputs":[],"source":["translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lj7NJxxAq5wy","outputId":"7efc175f-6073-4791-b023-75e22c095782"},"outputs":[{"data":{"text/plain":["'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["string.punctuation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZnFQA6tq5wz","outputId":"7d3570fc-e33c-4564-88cc-61330da942db"},"outputs":[{"data":{"text/plain":["'This project has four broad objectives  linked to feasible improvements in clean stove design and dissemination and their impacts on health and climate  1  assess the acceptability and availability of different stove technologies and fuels  2  experiment by varying stove price and social interactions among users to determine the impact of these variables on stove adoption rates  3  measure in situ the impacts of stove adoption on indoor and outdoor air pollution  and climate forcing  and 4  model the impacts of widespread stove adoption on regional and global climate through a range of scenarios directly informed by data from the field '"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# After\n","abstracts_list[0].translate(translator)"]},{"cell_type":"markdown","metadata":{"id":"Cpy-ti1Tq5wz"},"source":["### Tokenizing"]},{"cell_type":"markdown","metadata":{"id":"KyfuZU8Cq5wz"},"source":["We want to separate text into individual tokens (generally individual words). To do this, we will first write a function that takes a string and splits it up into individual words."]},{"cell_type":"markdown","metadata":{"id":"Pa-NV25eq5wz"},"source":["To tokenize, we will use a `.split()` function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGZDuEhzq5wz","outputId":"b435dd0f-202b-4660-f227-e53d51d9d574"},"outputs":[{"data":{"text/plain":["['This',\n"," 'project',\n"," 'has',\n"," 'four',\n"," 'broad',\n"," 'objectives,',\n"," 'linked',\n"," 'to',\n"," 'feasible',\n"," 'improvements']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["abstracts_list[0].split()[:10]"]},{"cell_type":"markdown","metadata":{"id":"01mkghK3q5w0"},"source":["We will do the whole process of removing punctuation, stemming, and tokenizing all in one function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aFMpNGiq5w0"},"outputs":[],"source":["def tokenize(text):\n","    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))  # translator that replaces punctuation with empty spaces\n","    return [stemmer.stem(i) for i in text.translate(translator).split()]  # stemmer and tokenizing into words"]},{"cell_type":"markdown","metadata":{"id":"ptImkKBFq5w0"},"source":["The `tokenize` function actually does several things at the same time. First, it removes any punctuation using the `translate` method. Then, the `split` method breaks it apart into individual words. Then, using `stemmer.stem`, it creates a list of the stemmed versions of each of those individual words."]},{"cell_type":"markdown","metadata":{"id":"GUrXYXCFq5w0"},"source":["Let's take a look at an example of how this works using the first abstract in our corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GNcGS8mq5w0","outputId":"3469d07a-c3a4-4aee-c7e2-6fb185da5a0d"},"outputs":[{"data":{"text/plain":["['this',\n"," 'project',\n"," 'has',\n"," 'four',\n"," 'broad',\n"," 'object',\n"," 'link',\n"," 'to',\n"," 'feasibl',\n"," 'improv',\n"," 'in',\n"," 'clean',\n"," 'stove',\n"," 'design',\n"," 'and',\n"," 'dissemin',\n"," 'and',\n"," 'their',\n"," 'impact',\n"," 'on']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tokenize(abstracts_list[0])[:20]"]},{"cell_type":"markdown","metadata":{"id":"Af5kwxjZq5w0"},"source":["What we get out of it is something called a **bag of words**. This is a list of all of the words that are in the abstract, cleaned of all punctuation and stemmed. The paragraph is now represented as as vector of individual words rather than as one whole entity."]},{"cell_type":"markdown","metadata":{"id":"4AmCTCNTq5w1"},"source":["We can apply this to each abstract in our corpus using `CountVectorizer`. This will not only do the tokenizing, but it will also count any duplicates of words and create a matrix that contains the frequency of each word. This will be quite a large matrix (number of columns will be number of unique words), so it outputs the data as a [sparse matrix](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/)."]},{"cell_type":"markdown","metadata":{"id":"Lt039Ofjq5w1"},"source":["We will first create the `vectorizer` object, and then fit it with our abstracts. This should give us back our overall corpus bag of words, as well as a list of features (that is, the unique words in all the abstracts)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4p_h4jTAq5w1"},"outputs":[],"source":["vectorizer = CountVectorizer(analyzer=\"word\",        # unit of features are single words rather than characters\n","                            tokenizer=tokenize,      # function to create tokens\n","                            ngram_range=(0,1),       # unigrams - single words\n","                            strip_accents='unicode', # remove accent characters\n","                            min_df = 0.05,           # only include words with minimum frequency of 0.05\n","                            max_df = 0.95)           # only include words with maximum frequency of 0.95"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7_8zQndeq5w1"},"outputs":[],"source":["bag_of_words = vectorizer.fit_transform(abstracts_list)  # transform our corpus as a bag of words\n","features = vectorizer.get_feature_names()"]},{"cell_type":"markdown","metadata":{"id":"7ufOkqXJq5w1"},"source":["Let's print some of these objects to get an idea of what they look like. We can save the resulting matrix into the dataframe format, which is familiar to us. Each column represents a feature name (unique word), and each row represents an abstract. As a result, we can see how often each of the words appear in each of the abstracts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCtDuNjEq5w1"},"outputs":[],"source":["bag_of_words_df = pd.DataFrame(bag_of_words.todense(), columns=features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xy4em_S-q5w1","outputId":"9b4bf62f-6246-4201-95e1-af35c27fc1ae"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>000</th>\n","      <th>1</th>\n","      <th>10</th>\n","      <th>12</th>\n","      <th>2</th>\n","      <th>20</th>\n","      <th>3</th>\n","      <th>30</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>...</th>\n","      <th>will</th>\n","      <th>with</th>\n","      <th>within</th>\n","      <th>without</th>\n","      <th>women</th>\n","      <th>work</th>\n","      <th>would</th>\n","      <th>year</th>\n","      <th>yet</th>\n","      <th>young</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>12</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>8</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows × 754 columns</p>\n","</div>"],"text/plain":["      000  1  10  12  2  20  3  30  4  5  ...  will  with  within  without  \\\n","0       0  1   0   0  1   0  1   0  1  0  ...     0     0       0        0   \n","1       0  0   0   0  0   0  0   0  1  1  ...     3     0       1        0   \n","2       0  4   0   6  3   0  1   0  1  0  ...    12     3       1        0   \n","3       0  1   0   0  1   0  1   0  1  1  ...     4     6       0        1   \n","4       0  2   1   1  3   0  2   2  2  0  ...     8     9       0        1   \n","...   ... ..  ..  .. ..  .. ..  .. .. ..  ...   ...   ...     ...      ...   \n","1995    0  1   0   0  0   0  0   0  0  0  ...     2     8       1        0   \n","1996    0  2   0   0  1   0  0   0  0  0  ...     6     0       0        0   \n","1997    0  0   0   0  0   0  0   0  0  0  ...     3     1       1        0   \n","1998    0  0   0   0  0   0  0   0  0  0  ...     4     2       0        0   \n","1999    0  1   0   0  2   0  2   0  1  0  ...     3     3       0        0   \n","\n","      women  work  would  year  yet  young  \n","0         0     0      0     0    0      0  \n","1         0     0      0     1    0      0  \n","2         0     0      0     0    0      0  \n","3         0     0      0     0    0      0  \n","4         0     0      0     5    0      3  \n","...     ...   ...    ...   ...  ...    ...  \n","1995      0     0      0     0    0      0  \n","1996      0     1      0     0    0      0  \n","1997      0     0      0     0    0      0  \n","1998      0     1      0     0    0      0  \n","1999      0     1      0     0    0      0  \n","\n","[2000 rows x 754 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["bag_of_words_df"]},{"cell_type":"markdown","metadata":{"id":"Lieci1A6q5w2"},"source":["Now that we have our bag of words, we can start using for models such as Latent Dirichlet Allocation."]},{"cell_type":"markdown","metadata":{"id":"jL8fTPvwq5w2"},"source":["### Practice: Load the abstracts of your interest and run the code above to get the bag of words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNveoN5Xq5w2"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9L2LI14Rq5w2"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WcR64zsq5w2"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kPx6v5oq5w2"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"3Y6r5e57q5w2"},"source":["## Latent Dirichlet Allocation"]},{"cell_type":"markdown","metadata":{"id":"UwpEM9TNq5w2"},"source":["Latent Dirichlet Allocation (LDA) is a statistical model that generates groups based on similarities. This is an example of an **unsupervised machine learning model**. That is, we don't have any sort of outcome variable - we are just trying to group the abstracts into rough categories."]},{"cell_type":"markdown","metadata":{"id":"WCBuTCP2q5w3"},"source":["Let's try fitting an LDA model. We first create a `LatentDirichletAllocation` object, then fit it using our corpus bag of words."]},{"cell_type":"markdown","metadata":{"id":"UAEaBa-nq5w3"},"source":["Note: we need to feed the `bag_of_words` variable (not the `bag_of_words_df` object as it represents a dataframe and we only created it to better understand the underlying matrix)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cd4bEVvVq5w3"},"outputs":[],"source":["lda = LatentDirichletAllocation(learning_method='online')\n","\n","doctopic = lda.fit_transform(bag_of_words)"]},{"cell_type":"markdown","metadata":{"id":"lzUj9ieuq5w3"},"source":["By default, LDA produces 10 topics."]},{"cell_type":"markdown","metadata":{"id":"DHkcnbadq5w3"},"source":["In order to view the top 5 words in each of 10 topics, we can do the following:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sd8J8ocQq5w3","outputId":"960464b1-ca51-4c96-c2e8-d350c96a6b25"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 disord, depress, mental, symptom, veteran\n","1 will, for, health, care, research\n","2 with, will, for, be, is\n","3 cell, that, is, will, we\n","4 for, this, will, develop, is\n","5 protein, for, that, structur, dna\n","6 infect, t, hiv, immun, cell\n","7 that, is, we, cell, will\n","8 research, train, program, diabet, mentor\n","9 will, that, with, is, brain\n"]}],"source":["ls_keywords = []\n","for i,topic in enumerate(lda.components_):  # lda.components_ represent topics\n","    word_idx = np.argsort(topic)[::-1][:5]\n","    keywords = ', '.join(features[i] for i in word_idx)\n","    ls_keywords.append(keywords)\n","    print(i, keywords)"]},{"cell_type":"markdown","metadata":{"id":"t_j2GPHFq5w3"},"source":["Not all of these look very helpful! There are way too many common words in the corpus, such as 'is', 'with', 'that' and so on. We need to remove them, because they don't actually have any interesting information about the documents."]},{"cell_type":"markdown","metadata":{"id":"MFp6TbDqq5w3"},"source":["### Removing meaningless text - Stopwords"]},{"cell_type":"markdown","metadata":{"id":"3sSX64i9q5w4"},"source":["Stopwords are words that are found commonly throughout a text and carry little semantic meaning. Examples of common stopwords are prepositions (\"to\", \"on\", \"in\"), articles (\"the\", \"an\", \"a\"), conjunctions (\"and\", \"or\", \"but\") and common nouns. For example, the words _the_ and _of_ are totally ubiquitous, so they won't serve as meaningful features, whether to distinguish documents from each other or to tell what a given document is about. You may also run into words that you want to remove based on where you obtained your corpus of text or what it is about. There are many lists of common stopwords available for you to use, both for general documents and for specific contexts, so you don't have to start from scratch."]},{"cell_type":"markdown","metadata":{"id":"A4-5heGqq5w4"},"source":["We can eliminate stopwords by checking all the words in our corpus against a list of commonly occuring stopwords that comes with NLTK."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxPOHmonq5w4","outputId":"0ffbe718-cc46-409e-b6b1-2a8603edded2"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\Ekaterina\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# We download stopwords\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCCikE2yq5w4"},"outputs":[],"source":["# Set the correct stop words\n","stop_words = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A5GSjCMlq5w4"},"outputs":[],"source":["# Tokenize stop words to match \n","stop_words = [tokenize(s)[0] for s in stop_words]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8b0fJkHuq5w4","outputId":"3fcb43ec-7c1c-432c-a8e9-97114f609b0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 cell, activ, aim, 1, protein\n","1 research, train, program, provid, develop\n","2 treatment, effect, studi, risk, patient\n","3 veteran, care, health, intervent, use\n","4 brain, use, function, studi, develop\n"]}],"source":["# Create Vectorizer\n","vectorizer = CountVectorizer(analyzer=\"word\",           # unit of features are single words rather than characters\n","                            tokenizer=tokenize,         # function to create tokens\n","                            ngram_range=(0,1),          # unigrams - single words\n","                            strip_accents='unicode',    # remove accent characters\n","                            stop_words = stop_words,    # remove stopwords\n","                            min_df = 0.05,              # only include words with minimum frequency of 0.05\n","                            max_df = 0.95)              # only include words with maximum frequency of 0.95\n","\n","# Creating bag of words\n","bag_of_words = vectorizer.fit_transform(abstracts_list) # transform our corpus as a bag of words\n","features = vectorizer.get_feature_names()               # get features (words)\n","\n","# Fitting LDA model\n","# Here we change number of topics to 5 topics\n","# n_components = 5\n","\n","lda = LatentDirichletAllocation(n_components = 5, learning_method='online')\n","doctopic = lda.fit_transform(bag_of_words)\n","\n","# Displaying the top keywords in each topic\n","ls_keywords = []\n","for i,topic in enumerate(lda.components_):\n","    word_idx = np.argsort(topic)[::-1][:5]\n","    keywords = ', '.join(features[i] for i in word_idx)\n","    ls_keywords.append(keywords)\n","    print(i, keywords)"]},{"cell_type":"markdown","metadata":{"id":"_THz4OdMq5w5"},"source":["### Practice: Change the number of topics for the abstracts of your interest (use the bag of words that you created in the previous section) and compare the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUQ-oTU2q5w5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cxik4JBzq5w5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vpk6cVR5q5w5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fn5jv2Voq5w5"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"wdJ1PuMkq5w5"},"source":["### N-grams - Adding context by creating N-grams"]},{"cell_type":"markdown","metadata":{"id":"9IIYx6mfq5w5"},"source":["Obviously, reducing a document to a bag of words means losing much of its meaning - we put words in certain orders, and group words together in phrases and sentences, precisely to give  them more meaning. If you follow the processing steps we have gone through so far, splitting your document into individual words and then removing stopwords, you will completely lose all phrases like \"commander in chief\"."]},{"cell_type":"markdown","metadata":{"id":"kxMxzpH7q5w5"},"source":["One way to address this is to break down each document similarly, but rather than treating each word as an individual unit, treat each group of 2 words, or 3 words, or _n_ words, as a unit. We call is a \"bag on _n_-grams\", where _n_ is the number of words in each chunk. Then you can analyze which groups of words commonly occur together (in a fixed order)."]},{"cell_type":"markdown","metadata":{"id":"n8ft9eg7q5w5"},"source":["We will need to change the `ngram_range` variable from `(0,1)` to `(0,2)` to get the  bigrams, to `(0,3)` to get the trigrams, and so on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"toaQleqXq5w5"},"outputs":[],"source":["vectorizer = CountVectorizer(analyzer=\"word\",           # unit of features are single words rather than characters\n","                            tokenizer=tokenize,         # function to create tokens\n","                            ngram_range=(0,2),          # allow for bigrams\n","                            strip_accents='unicode',    # remove accent characters\n","                            stop_words = stop_words,    # remove stopwords\n","                            min_df = 0.05,              # only include words with minimum frequency of 0.05\n","                            max_df = 0.95)              # only include words with maximum frequency of 0.95"]},{"cell_type":"markdown","metadata":{"id":"jrsD4NETq5w6"},"source":["Note that it's entirely possible to not have any bigrams or other n-grams. This just means that there aren't many very common phrases in the corpus."]},{"cell_type":"markdown","metadata":{"id":"2HjK2bvRq5w6"},"source":["## TF-IDF - Weighting terms based on frequency"]},{"cell_type":"markdown","metadata":{"id":"Nzgf_TA5q5w6"},"source":["A final step in cleaning and processing our text data is **Term Frequency-Inverse Document Frequency (TF-IDF)**. TF-IDF is based on the idea that the words (or terms) that are most related to a certain topic will occur frequently in documents on that topic, and infrequently in unrelated documents. TF-IDF re-weights words so that we emphasize words that are unique to a document and suppress words that are common throughout the corpus by inversely weighting terms based on their frequency within the document and across the corpus."]},{"cell_type":"markdown","metadata":{"id":"w5eadPsDq5w6"},"source":["We can also try adding to the stopwords list other stopwords that you think might be useful to remove in order to create more meaningful topics:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wny8_CP6q5w6"},"outputs":[],"source":["stop = stop_words + ['provid', 'use', 'studi']\n","full_stopwords = [tokenize(s)[0] for s in stop]"]},{"cell_type":"markdown","metadata":{"id":"7dcsRawwq5w6"},"source":["Let's look at how to use TF-IDF:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njc8cOgfq5w6","outputId":"34e6a48e-62b3-4963-f114-b4ee124ddff9"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 care, veteran, health, intervent, patient\n","1 cell, protein, gene, infect, genet\n","2 research, train, core, program, career\n","3 cell, induc, mice, express, activ\n","4 brain, cognit, behavior, age, measur\n"]}],"source":["vectorizer = CountVectorizer(analyzer=\"word\",            # unit of features are single words rather than characters\n","                            tokenizer=tokenize,          # function to create tokens\n","                            ngram_range=(0,2),           # allow for bigrams\n","                            strip_accents='unicode',     # remove accent characters\n","                            stop_words = full_stopwords, # remove stopwords\n","                            min_df = 0.05,               # only include words with minimum frequency of 0.05\n","                            max_df = 0.95)               # only include words with maximum frequency of 0.95\n","\n","# Creating bag of words\n","bag_of_words = vectorizer.fit_transform(abstracts_list)  # transform our corpus as a bag of words\n","features = vectorizer.get_feature_names()                # get features (words)\n","\n","# Use TfidfTransformer to re-weight bag of words\n","transformer = TfidfTransformer(norm = None, smooth_idf = True, sublinear_tf = True)\n","tfidf = transformer.fit_transform(bag_of_words)\n","\n","# Fitting LDA model\n","lda = LatentDirichletAllocation(n_components = 5, learning_method='online')\n","doctopic = lda.fit_transform(tfidf)\n","\n","# Displaying the top keywords in each topic\n","ls_keywords = []\n","for i,topic in enumerate(lda.components_):\n","    word_idx = np.argsort(topic)[::-1][:5]\n","    keywords = ', '.join(features[i] for i in word_idx)\n","    ls_keywords.append(keywords)\n","    print(i, keywords)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PuchlaUEq5w6","outputId":"90a058f3-feda-4e1d-861c-1e071365e396"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>care, veteran, health, intervent, patient</th>\n","      <th>cell, protein, gene, infect, genet</th>\n","      <th>research, train, core, program, career</th>\n","      <th>cell, induc, mice, express, activ</th>\n","      <th>brain, cognit, behavior, age, measur</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.827101</td>\n","      <td>0.002254</td>\n","      <td>0.166191</td>\n","      <td>0.002214</td>\n","      <td>0.002240</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.871166</td>\n","      <td>0.000894</td>\n","      <td>0.126159</td>\n","      <td>0.000888</td>\n","      <td>0.000893</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.998342</td>\n","      <td>0.000413</td>\n","      <td>0.000412</td>\n","      <td>0.000416</td>\n","      <td>0.000417</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.853308</td>\n","      <td>0.000485</td>\n","      <td>0.000484</td>\n","      <td>0.113621</td>\n","      <td>0.032102</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.040252</td>\n","      <td>0.000349</td>\n","      <td>0.131577</td>\n","      <td>0.000347</td>\n","      <td>0.827474</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1995</th>\n","      <td>0.000596</td>\n","      <td>0.469685</td>\n","      <td>0.000597</td>\n","      <td>0.528521</td>\n","      <td>0.000602</td>\n","    </tr>\n","    <tr>\n","      <th>1996</th>\n","      <td>0.000556</td>\n","      <td>0.126623</td>\n","      <td>0.000559</td>\n","      <td>0.871698</td>\n","      <td>0.000564</td>\n","    </tr>\n","    <tr>\n","      <th>1997</th>\n","      <td>0.000614</td>\n","      <td>0.997549</td>\n","      <td>0.000613</td>\n","      <td>0.000613</td>\n","      <td>0.000612</td>\n","    </tr>\n","    <tr>\n","      <th>1998</th>\n","      <td>0.031484</td>\n","      <td>0.929620</td>\n","      <td>0.037109</td>\n","      <td>0.000894</td>\n","      <td>0.000894</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>0.054320</td>\n","      <td>0.141507</td>\n","      <td>0.000518</td>\n","      <td>0.803134</td>\n","      <td>0.000521</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2000 rows × 5 columns</p>\n","</div>"],"text/plain":["      care, veteran, health, intervent, patient  \\\n","0                                      0.827101   \n","1                                      0.871166   \n","2                                      0.998342   \n","3                                      0.853308   \n","4                                      0.040252   \n","...                                         ...   \n","1995                                   0.000596   \n","1996                                   0.000556   \n","1997                                   0.000614   \n","1998                                   0.031484   \n","1999                                   0.054320   \n","\n","      cell, protein, gene, infect, genet  \\\n","0                               0.002254   \n","1                               0.000894   \n","2                               0.000413   \n","3                               0.000485   \n","4                               0.000349   \n","...                                  ...   \n","1995                            0.469685   \n","1996                            0.126623   \n","1997                            0.997549   \n","1998                            0.929620   \n","1999                            0.141507   \n","\n","      research, train, core, program, career  \\\n","0                                   0.166191   \n","1                                   0.126159   \n","2                                   0.000412   \n","3                                   0.000484   \n","4                                   0.131577   \n","...                                      ...   \n","1995                                0.000597   \n","1996                                0.000559   \n","1997                                0.000613   \n","1998                                0.037109   \n","1999                                0.000518   \n","\n","      cell, induc, mice, express, activ  brain, cognit, behavior, age, measur  \n","0                              0.002214                              0.002240  \n","1                              0.000888                              0.000893  \n","2                              0.000416                              0.000417  \n","3                              0.113621                              0.032102  \n","4                              0.000347                              0.827474  \n","...                                 ...                                   ...  \n","1995                           0.528521                              0.000602  \n","1996                           0.871698                              0.000564  \n","1997                           0.000613                              0.000612  \n","1998                           0.000894                              0.000894  \n","1999                           0.803134                              0.000521  \n","\n","[2000 rows x 5 columns]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# Look at how each abstract (row) fits into each topic (column)\n","# A score is assigned to each of the topic (the higher the score, the more likely the abstract belongs to that topic)\n","pd.DataFrame(doctopic, columns = ls_keywords)"]},{"cell_type":"markdown","metadata":{"id":"ONYTZ1Tkq5w7"},"source":["### Map it back to get the Project ID"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"2XXaoAcjq5w7","executionInfo":{"status":"error","timestamp":1650911089619,"user_tz":240,"elapsed":357,"user":{"displayName":"Max Magid","userId":"00651427591879495055"}},"outputId":"8618ae23-e88a-4977-e3a0-33b7b5ce90b9","colab":{"base_uri":"https://localhost:8080/","height":166}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c1d2786eed6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopics_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoctopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mls_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["topics_doc = pd.DataFrame(doctopic, columns = ls_keywords)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZxm8Gv7q5w7","executionInfo":{"status":"aborted","timestamp":1650911089615,"user_tz":240,"elapsed":172,"user":{"displayName":"Max Magid","userId":"00651427591879495055"}}},"outputs":[],"source":["topics_doc.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSRS02pkq5w7","executionInfo":{"status":"aborted","timestamp":1650911089617,"user_tz":240,"elapsed":174,"user":{"displayName":"Max Magid","userId":"00651427591879495055"}}},"outputs":[],"source":["abstracts.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scC8KRPiq5w7","executionInfo":{"status":"aborted","timestamp":1650911089618,"user_tz":240,"elapsed":174,"user":{"displayName":"Max Magid","userId":"00651427591879495055"}}},"outputs":[],"source":["doc_topics_project_id = pd.concat([topics_doc,abstracts],axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAsjj5z8q5w7","executionInfo":{"status":"aborted","timestamp":1650911089618,"user_tz":240,"elapsed":5,"user":{"displayName":"Max Magid","userId":"00651427591879495055"}}},"outputs":[],"source":["doc_topics_project_id.head()"]},{"cell_type":"markdown","metadata":{"id":"uFeJG6V_q5w7"},"source":["### Choose one topic per document with the highest score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBc3LwDJq5w8","outputId":"faaf53c8-bd55-4591-b0c2-6ad8dd378bdf"},"outputs":[{"data":{"text/plain":["0       care, veteran, health, intervent, patient\n","1       care, veteran, health, intervent, patient\n","2       care, veteran, health, intervent, patient\n","3       care, veteran, health, intervent, patient\n","4            brain, cognit, behavior, age, measur\n","                          ...                    \n","1995            cell, induc, mice, express, activ\n","1996            cell, induc, mice, express, activ\n","1997           cell, protein, gene, infect, genet\n","1998           cell, protein, gene, infect, genet\n","1999            cell, induc, mice, express, activ\n","Length: 2000, dtype: object"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# Idxmax function and axis=1: return the column name of the max value in a row\n","topics_doc.idxmax(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HV4_XcGuq5w8"},"outputs":[],"source":["doc_topics_project_id_with_max = pd.concat([topics_doc.idxmax(axis=1),abstracts],axis=1).rename(columns={0:'topic'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqdKXzRZq5w8","outputId":"bd027604-e715-4d6f-f131-58a7c6bc9da6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>topic</th>\n","      <th>PROJECT_ID</th>\n","      <th>ABSTRACT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>673465</td>\n","      <td>This project has four broad objectives, linked...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725488</td>\n","      <td>Project Summary/Abstract In our application, A...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725489</td>\n","      <td>Anticipated Impacts on Veteran's Healthcare: C...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725490</td>\n","      <td>Background. Given the growing strain on VAHCS ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>brain, cognit, behavior, age, measur</td>\n","      <td>725491</td>\n","      <td>DESCRIPTION:       Temporal coding is the abil...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       topic  PROJECT_ID  \\\n","0  care, veteran, health, intervent, patient      673465   \n","1  care, veteran, health, intervent, patient      725488   \n","2  care, veteran, health, intervent, patient      725489   \n","3  care, veteran, health, intervent, patient      725490   \n","4       brain, cognit, behavior, age, measur      725491   \n","\n","                                            ABSTRACT  \n","0  This project has four broad objectives, linked...  \n","1  Project Summary/Abstract In our application, A...  \n","2  Anticipated Impacts on Veteran's Healthcare: C...  \n","3  Background. Given the growing strain on VAHCS ...  \n","4  DESCRIPTION:       Temporal coding is the abil...  "]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["doc_topics_project_id_with_max.head()"]},{"cell_type":"markdown","metadata":{"id":"g5n-Iyo9q5w8"},"source":["Let's see more of the abstracts:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4RNMWs3Jq5w8","outputId":"9413e7cf-0814-47ba-87a3-095576a72081"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>topic</th>\n","      <th>PROJECT_ID</th>\n","      <th>ABSTRACT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>673465</td>\n","      <td>This project has four broad objectives, linked to feasible improvements in clean stove design and dissemination and their impacts on health and climate: 1) assess the acceptability and availabilit...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725488</td>\n","      <td>Project Summary/Abstract In our application, Aligning Resources for the Care of Homeless Veterans (ARCH) we propose studying ways to best organize and deliver primary care for homeless Veterans. W...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725489</td>\n","      <td>Anticipated Impacts on Veteran's Healthcare: Cardiovascular disease (CVD) is the leading cause of death in the U.S.; more than 80% of veterans have &gt; 2 risk factors for CVD. An intervention that a...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725490</td>\n","      <td>Background. Given the growing strain on VAHCS resulting from increasing caseloads of cancer patients,interventions are badly needed that assist patients in managing their illness, improve quality ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>brain, cognit, behavior, age, measur</td>\n","      <td>725491</td>\n","      <td>DESCRIPTION:       Temporal coding is the ability of the auditory system to detect and transmit fluctuations in time across the acoustic signal. Sensorineural hearing loss and early aging have the...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725492</td>\n","      <td>DESCRIPTION       Design: This four year study employs mixed methods, in four phases. The purpose is to complete psychometric testing of a low-burden, valid and reliable instrument to measure whee...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725493</td>\n","      <td>DESCRIPTION (provided by applicant):    Anticipated Impacts on Participant's Healthcare: This planned evaluation of our stroke self- management program may improve the patient's healthcare by esta...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725494</td>\n","      <td>DESCRIPTION (provided by applicant):       In keeping with the guidelines and points of emphasis established by the VHA Uniform Mental Health Services Code, the third submission of this research s...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725495</td>\n","      <td>DESCRIPTION (provided by applicant):    The purpose of this research project is to determine if Fatigue: Take Control (FTC), a formal multi-modal program owned and distributed by the National Mult...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>care, veteran, health, intervent, patient</td>\n","      <td>725496</td>\n","      <td>DESCRIPTION (provided by applicant):       Our main objective is to conduct a type I hybrid effectiveness/implementation study to test the effectiveness of a successfully-piloted, evidence-based, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       topic  PROJECT_ID  \\\n","0  care, veteran, health, intervent, patient      673465   \n","1  care, veteran, health, intervent, patient      725488   \n","2  care, veteran, health, intervent, patient      725489   \n","3  care, veteran, health, intervent, patient      725490   \n","4       brain, cognit, behavior, age, measur      725491   \n","5  care, veteran, health, intervent, patient      725492   \n","6  care, veteran, health, intervent, patient      725493   \n","7  care, veteran, health, intervent, patient      725494   \n","8  care, veteran, health, intervent, patient      725495   \n","9  care, veteran, health, intervent, patient      725496   \n","\n","                                                                                                                                                                                                  ABSTRACT  \n","0  This project has four broad objectives, linked to feasible improvements in clean stove design and dissemination and their impacts on health and climate: 1) assess the acceptability and availabilit...  \n","1  Project Summary/Abstract In our application, Aligning Resources for the Care of Homeless Veterans (ARCH) we propose studying ways to best organize and deliver primary care for homeless Veterans. W...  \n","2  Anticipated Impacts on Veteran's Healthcare: Cardiovascular disease (CVD) is the leading cause of death in the U.S.; more than 80% of veterans have > 2 risk factors for CVD. An intervention that a...  \n","3  Background. Given the growing strain on VAHCS resulting from increasing caseloads of cancer patients,interventions are badly needed that assist patients in managing their illness, improve quality ...  \n","4  DESCRIPTION:       Temporal coding is the ability of the auditory system to detect and transmit fluctuations in time across the acoustic signal. Sensorineural hearing loss and early aging have the...  \n","5  DESCRIPTION       Design: This four year study employs mixed methods, in four phases. The purpose is to complete psychometric testing of a low-burden, valid and reliable instrument to measure whee...  \n","6  DESCRIPTION (provided by applicant):    Anticipated Impacts on Participant's Healthcare: This planned evaluation of our stroke self- management program may improve the patient's healthcare by esta...  \n","7  DESCRIPTION (provided by applicant):       In keeping with the guidelines and points of emphasis established by the VHA Uniform Mental Health Services Code, the third submission of this research s...  \n","8  DESCRIPTION (provided by applicant):    The purpose of this research project is to determine if Fatigue: Take Control (FTC), a formal multi-modal program owned and distributed by the National Mult...  \n","9  DESCRIPTION (provided by applicant):       Our main objective is to conduct a type I hybrid effectiveness/implementation study to test the effectiveness of a successfully-piloted, evidence-based, ...  "]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["pd.set_option('display.max_colwidth', 200)\n","doc_topics_project_id_with_max.head(10)"]},{"cell_type":"markdown","metadata":{"id":"Na0hAJNcq5w8"},"source":["### Assignment 4 (due 03/29/2022)\n","\n","### Apply text analysis to the abstracts of your interest - create topics (experiment with different number of topics, cleaning the text, removing stopwords, etc. and choose the best approach for your abstracts). Submit a Jupyter notebook with your analysis (individual submission)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2t9av_pJq5w8"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TPDgDt_Zq5w8"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrHwyGSHq5w9"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kI6oVsPXq5w9"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3lUiIDRq5w9"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"uqiBiMnWq5w9"},"source":["### Further Resources"]},{"cell_type":"markdown","metadata":{"id":"P28I8LX8q5w9"},"source":["A great resource for NLP in Python is Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit."]},{"cell_type":"markdown","metadata":{"id":"geBb2kpwq5w9"},"source":["Some of the most popular and useful Python libraries for NLP: \n","- NLTK and the associated book/tutorial (www.nltk.org/book), a good introduction to different NLP tasks.\n","- TextBlob, a simplified library for various NLP tasks (noun phrase extraction, part-of-speech tagging, sentiment analysis, classification, tokenization, etc.).\n","- spaCy (named entity recognition, part-of-speech tagging, dependency parsing, tokenization, etc.).\n","- Stanford CoreNLP (while NLTK's emphasis is on simple reference implementations, Stanford CoreNLP is focused no fast implementations of cutting-edge algorithms, particularly for syntactic analysis, e.g. determining the subject of a sentence)."]},{"cell_type":"markdown","metadata":{"id":"H_MDfs_4q5w9"},"source":["### Supervised Learning: Document Classification"]},{"cell_type":"markdown","metadata":{"id":"2eRfnsvwq5w9"},"source":["Topic modeling is an example of an unsupervised learning: we were looking to uncover structure in the form of topics, but we did not necessarily know the ground truth of how many topics there are."]},{"cell_type":"markdown","metadata":{"id":"I2x25NdFq5w9"},"source":["We can also do supervised learning with text data. In supervised learning, we have a _known_ outcome or label (_Y_) that we want to produce given some data (_X_), and in general, we want to be able to produce this _Y_ when we _don't_ know it, or when we _only_ have _X_."]},{"cell_type":"markdown","metadata":{"id":"8xiYQuScq5w9"},"source":["In order to produce labels we need to first have examples our algorithm can learn from, a \"training set\". In the context of text analysis, developing a training set can be very expensive, as it can require a large amount of human labor or linguistic expertise. **Document classification** is an example of supervised learning in which we want to characterize our documents based on their contents (_X_). A common example of document classification is spam e-mail detection. Another example of supervised learning in text analysis is _sentimenet analysis_, where _X_ is our documents and _Y_ is the state of the author. This \"state\" is dependent on the question you are trying to answer, and can range from the author being happy or unhappy with a product to the author being politically conservative or liberal. Another example is _part-of-speech tagging_ where _X_ are individual words and _Y_ is the part-of-speech."]},{"cell_type":"markdown","metadata":{"id":"Ll3QhrCCq5w-"},"source":["In the context of our class, one useful application is, for example, classifying whether a text belongs to a given field of study (e.g. broad fields of study used in the SDR reporting). In order to create a text classification model, just like we will do in the Machine Learning notebooks, you would need to have a training dataset with text and associated field of study label, which you can then use to train your model on and predict the label (field of study) of new, unseen, text."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Copy of Text_Analysis_notebook.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}